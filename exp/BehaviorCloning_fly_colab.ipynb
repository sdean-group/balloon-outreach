{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a1d6230",
      "metadata": {
        "id": "3a1d6230"
      },
      "source": [
        "# Behavior Cloning\n",
        "\n",
        "## Objective: Fly balloon as far as possible.\n",
        "\n",
        "\n",
        "0. You should **download** your ERA5 wind field data from the following link:\n",
        "https://drive.google.com/file/d/1Y5ocrCH0TVf9ZDkVGaJ03ftUrtKEVxIf/view?usp=sharing\n",
        "- Download the data and save it into your Google Drive.\n",
        "\n",
        "1.Imitation learning is a paradigm where an agent learns to mimic an **expert's behavior** instead of learning purely from trial-and-error rewards (e.g., reinforcement learning).\n",
        "\n",
        "- The simplest common approach is called \"**behavior cloning**\" (BC). It treats the expert's demonstrations as training data samples; train a policy using the expert's input(state) and output(action) pairs via supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eMuw3vQKXQfn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMuw3vQKXQfn",
        "outputId": "d454827a-4cb7-4589-c3c7-44f952db2a36"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/sdean-group/balloon-outreach.git\n",
        "%cd balloon-outreach\n",
        "!git checkout v2_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Th2NSz-qXYJ6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th2NSz-qXYJ6",
        "outputId": "361e4db7-88d7-4569-c9ae-50b9ba4a046b"
      },
      "outputs": [],
      "source": [
        "!pip install xarray==2025.4.0\n",
        "!pip install netCDF4\n",
        "!pip install opensimplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Upl_V1XXamM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Upl_V1XXamM",
        "outputId": "46d1c0d0-e20a-418d-d8bd-7b547e405ba5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AyVuxgeLXdUd",
      "metadata": {
        "id": "AyVuxgeLXdUd"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/balloon-outreach/')\n",
        "\n",
        "datapath = \"/content/drive/MyDrive/era5_data.nc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed7d414",
      "metadata": {
        "id": "0ed7d414"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from env.balloon_env import BalloonERAEnvironment\n",
        "from agent.mppi_agent import MPPIAgentWithCostFunction, MPPIAgent\n",
        "from exp.learning_util import plot_expert_summary, plot_agent_summary\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65b113a",
      "metadata": {
        "id": "f65b113a"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D4dVV1lfidIq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4dVV1lfidIq",
        "outputId": "c59de990-591a-46cb-e553-08290e7569f7"
      },
      "outputs": [],
      "source": [
        "ds          = xr.open_dataset(datapath, engine=\"netcdf4\")\n",
        "start_time  = dt.datetime(year=2024, month=7, day=1, hour=0, minute=0)\n",
        "\n",
        "\n",
        "#This is Ithaca\n",
        "initial_lat = 42.6\n",
        "initial_lon = -76.5\n",
        "initial_alt = 10.0\n",
        "\n",
        "target_lat = None\n",
        "target_lon = None\n",
        "target_alt = None\n",
        "\n",
        "time_step = 120 #120 seconds\n",
        "objective = 'fly'\n",
        "\n",
        "env = BalloonERAEnvironment(ds=ds,\n",
        "                            start_time=start_time,\n",
        "                            initial_lat=initial_lat,\n",
        "                            initial_lon=initial_lon,\n",
        "                            initial_alt=initial_alt,\n",
        "                            target_lat=target_lat,\n",
        "                            target_lon=target_lon,\n",
        "                            target_alt=target_alt,\n",
        "                            objective=objective,\n",
        "                            dt=time_step,\n",
        "                            viz=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0efe112",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0efe112",
        "outputId": "c64f246f-0fd8-4ecb-c602-adf1a9a57644"
      },
      "outputs": [],
      "source": [
        "env.wind_field.disable_noise()\n",
        "initial_state = env.reset()\n",
        "print(initial_state) # 21-dimensional vector\n",
        "\n",
        "# state = (latitude, longitude, altitude, volume/max_volume, sand/max_sand,\n",
        "# vertical velocity, current_time, uv vectors in 7 different pressure levels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3baddc16",
      "metadata": {
        "id": "3baddc16"
      },
      "source": [
        "## Define expert policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021b3be4",
      "metadata": {
        "id": "021b3be4"
      },
      "outputs": [],
      "source": [
        "# Parameters for MPPI agent\n",
        "\n",
        "max_steps = int(1440/(time_step/60)) #1 day\n",
        "\n",
        "num_samples=10\n",
        "acc_bounds= (-0.1, 0.1)\n",
        "noise_std = 0.1\n",
        "num_iterations=1\n",
        "\n",
        "# For fly\n",
        "horizon = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600e9012",
      "metadata": {
        "id": "600e9012"
      },
      "outputs": [],
      "source": [
        "expert = MPPIAgentWithCostFunction(target_lat=target_lat,\n",
        "                                   target_lon=target_lon,\n",
        "                                   target_alt=target_alt,\n",
        "                                   num_samples=num_samples,\n",
        "                                   acc_bounds= acc_bounds,\n",
        "                                   noise_std=noise_std,\n",
        "                                   num_iterations=num_iterations,\n",
        "                                   horizon=horizon,\n",
        "                                   visualize=False,\n",
        "                                   objective=objective)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QnRc7mizo7MD",
      "metadata": {
        "id": "QnRc7mizo7MD"
      },
      "source": [
        "## Collect expert state-action pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0092a54",
      "metadata": {
        "id": "a0092a54"
      },
      "outputs": [],
      "source": [
        "# Run an episode from an expert and collect its behavior\n",
        "\n",
        "def run_expert_episode(\n",
        "        env: BalloonERAEnvironment,\n",
        "        agent:MPPIAgent,\n",
        "        max_steps: int = 100,\n",
        "        policy_name: str='expert'):\n",
        "    \"\"\"\n",
        "    Run one episode with the given agent,\n",
        "    collect state-action pair of the agent,\n",
        "    and plot its trajectory in the given environment.\n",
        "    \"\"\"\n",
        "    # Save state-action pairs from expert policy\n",
        "    initial_states = []\n",
        "    initial_actions = []\n",
        "\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    # Store trajectory for plotting\n",
        "    trajectory = [(state[0], state[1])]  # (lat, lon) pairs\n",
        "    altitudes = [state[2]]  # Store altitudes\n",
        "\n",
        "    actions = []\n",
        "    velocities = []\n",
        "    helium_mass = []\n",
        "    sands = []\n",
        "    for step in range(max_steps):\n",
        "        # Get action from agent\n",
        "        action = agent.select_action(state, env, step)\n",
        "\n",
        "        # record state and expert action\n",
        "        initial_states.append(state)\n",
        "        initial_actions.append(action)\n",
        "\n",
        "        # Take step\n",
        "        state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        actions.append(float(action[0]) if isinstance(action, np.ndarray) else float(action))\n",
        "        velocities.append(env.balloon.vertical_velocity)\n",
        "        helium_mass.append(env.balloon.helium_mass)\n",
        "        sands.append(env.balloon.sand)\n",
        "\n",
        "        # Store position and altitude\n",
        "        trajectory.append((state[0], state[1]))\n",
        "        altitudes.append(state[2])\n",
        "        print(f\"Step {step}: lat: {state[0]:.2f}, lon: {state[1]:.2f}, alt: {state[2]:.2f}\")\n",
        "        # env.render()\n",
        "\n",
        "        if done:\n",
        "            print(f\"\\nEpisode terminated: {info}\")\n",
        "            break\n",
        "\n",
        "    # Convert to arrays\n",
        "    states_np = np.array(initial_states, dtype=np.float32)\n",
        "    actions_np = np.array(initial_actions, dtype=np.float32)\n",
        "    print(f\"Collected {len(initial_states)} state-action pairs from expert.\")\n",
        "\n",
        "    # --- Combined 2x2 Summary Plot ---\n",
        "    plot_expert_summary(agent, trajectory, altitudes, actions, velocities, helium_mass, sands, policy_name, max_steps, time_step)\n",
        "\n",
        "    return total_reward, states_np, actions_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38454753",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38454753",
        "outputId": "8d6d54db-0ab8-458a-a4f4-960f5d894757"
      },
      "outputs": [],
      "source": [
        "env.wind_field.disable_noise()\n",
        "# env.wind_field.enable_noise(noise_seed=10) # If you want to enable noisy wind field\n",
        "expert_total_reward_list = []\n",
        "expert_states_list = []\n",
        "expert_actions_list = []\n",
        "num_iter = 3\n",
        "for i in range(num_iter):\n",
        "    expert_total_reward, expert_states_np, expert_actions_np = run_expert_episode(env, expert, max_steps=max_steps, policy_name=f'expert_{i+1}')\n",
        "    expert_total_reward_list.append(expert_total_reward)\n",
        "    expert_states_list.append(expert_states_np)\n",
        "    expert_actions_list.append(expert_actions_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85684cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85684cf",
        "outputId": "3dd169a8-ebd5-4a0a-dfb3-fa554f066edb"
      },
      "outputs": [],
      "source": [
        "print(expert_total_reward_list)\n",
        "expert_avg_total_reward = sum(expert_total_reward_list)/len(expert_total_reward_list)\n",
        "print(f\"Expert Trajectory reward in average: {expert_avg_total_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d967f346",
      "metadata": {
        "id": "d967f346"
      },
      "outputs": [],
      "source": [
        "expert_states_np = np.concatenate(expert_states_list)\n",
        "expert_actions_np = np.concatenate(expert_actions_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e075362",
      "metadata": {
        "id": "2e075362"
      },
      "source": [
        "## Define our policy network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cbfa56c",
      "metadata": {
        "id": "1cbfa56c"
      },
      "outputs": [],
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, input_dim=21, hidden_dim=64, output_dim=1):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim*2)\n",
        "        self.ln1 = nn.LayerNorm(hidden_dim*2)\n",
        "        self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Weight initialization\n",
        "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight, gain=nn.init.calculate_gain('tanh'))\n",
        "        nn.init.zeros_(self.fc3.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, input_dim)\n",
        "        x = self.fc1(x)\n",
        "        x = self.ln1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.ln2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return F.tanh(x)\n",
        "\n",
        "# Initialize policy network and optimizer\n",
        "policy = PolicyNet()\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
        "loss_fn = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d47f611",
      "metadata": {
        "id": "1d47f611"
      },
      "outputs": [],
      "source": [
        "# Evaluate a policy on the environment\n",
        "def evaluate_policy(env: BalloonERAEnvironment,\n",
        "                    policy: nn.Module,\n",
        "                    objective: str,\n",
        "                    max_steps: int,\n",
        "                    policy_name: str,\n",
        "                    expert_avg_total_reward: float):\n",
        "    policy.eval()\n",
        "    if policy.training:\n",
        "        print(\"→ policy is in training mode\")\n",
        "    else:\n",
        "        print(\"→ policy is in evaluation mode\")\n",
        "\n",
        "    state = env.reset()\n",
        "    total_reward = 0.0\n",
        "\n",
        "    # Store trajectory for plotting\n",
        "    trajectory = [(state[0], state[1])]  # (lat, lon) pairs\n",
        "    altitudes = [state[2]]  # Store altitudes\n",
        "\n",
        "    actions = []\n",
        "    velocities = []\n",
        "    helium_mass = []\n",
        "    sands = []\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)      # shape (1, 21)\n",
        "        action_pred = policy(state_tensor).item()                        # continuous action\n",
        "        state, reward, done, info = env.step(action_pred)\n",
        "        total_reward += reward\n",
        "\n",
        "        actions.append(float(action_pred) if isinstance(action_pred, np.ndarray) else float(action_pred))\n",
        "        velocities.append(env.balloon.vertical_velocity)\n",
        "        helium_mass.append(env.balloon.helium_mass)\n",
        "        sands.append(env.balloon.sand)\n",
        "\n",
        "        # Store position and altitude\n",
        "        trajectory.append((state[0], state[1]))\n",
        "        altitudes.append(state[2])\n",
        "\n",
        "        if done:\n",
        "            print(f\"\\nEpisode terminated: {info}\")\n",
        "            break\n",
        "    print(f\"Total reward obtained from current policy: {total_reward:.2f}\")\n",
        "    print(f\"Expert policy reward - Current policy reward: {expert_avg_total_reward-total_reward:.2f}\")\n",
        "\n",
        "    # Plot final agent summary\n",
        "    plot_agent_summary(\n",
        "        trajectory,\n",
        "        altitudes,\n",
        "        actions,\n",
        "        velocities,\n",
        "        helium_mass,\n",
        "        sands,\n",
        "        policy_name,\n",
        "        max_steps,\n",
        "        env.dt,\n",
        "        objective\n",
        "        )\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21bd26c1",
      "metadata": {
        "id": "21bd26c1"
      },
      "outputs": [],
      "source": [
        "env.wind_field.disable_noise()\n",
        "# env.wind_field.enable_noise(noise_seed=10) # If you want to enable noisy wind field\n",
        "evaluate_policy(env, policy, objective=objective, max_steps=max_steps, policy_name='random_policy_noiseless', expert_avg_total_reward=expert_avg_total_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41048dd7",
      "metadata": {
        "id": "41048dd7"
      },
      "source": [
        "## Behavior Cloning (Supervised Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09b61f3",
      "metadata": {
        "id": "f09b61f3"
      },
      "outputs": [],
      "source": [
        "# === Initial Dataset & DataLoader Construction ===\n",
        "\n",
        "# Convert to tensors\n",
        "states_tensor = torch.from_numpy(expert_states_np).float()               # shape (N, 21)\n",
        "actions_tensor = torch.from_numpy(expert_actions_np).float()             # shape (N, 1)\n",
        "\n",
        "# Construct TensorDataset and DataLoader\n",
        "dataset = TensorDataset(states_tensor, actions_tensor)\n",
        "loader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e749d404",
      "metadata": {
        "id": "e749d404"
      },
      "outputs": [],
      "source": [
        "# === Training Function Using Loader ===\n",
        "def train_one_epoch(loader, policy, optimizer, loss_fn):\n",
        "    policy.train()\n",
        "    if policy.training:\n",
        "        print(\"→ policy is in training mode\")\n",
        "    else:\n",
        "        print(\"→ policy is in evaluation mode\")\n",
        "\n",
        "    total_loss = 0.0\n",
        "    for x_batch, y_batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        preds = policy(x_batch)\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x_batch.size(0)\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43e530b4",
      "metadata": {
        "id": "43e530b4"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "for epoch in range(10):\n",
        "    avg_loss = train_one_epoch(loader, policy, optimizer, loss_fn)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/10 — Avg Loss: {avg_loss:.6f} \\n\")\n",
        "\n",
        "# plot the loss curve from behavior cloning\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(range(1, len(losses)+1), losses, marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Training Loss')\n",
        "plt.title('Behavior Cloning Loss Curve')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffa43e9",
      "metadata": {
        "id": "8ffa43e9"
      },
      "outputs": [],
      "source": [
        "env.wind_field.disable_noise()\n",
        "# env.wind_field.enable_noise(noise_seed=10) # If you want to enable noisy wind field\n",
        "evaluate_policy(env, policy, max_steps=max_steps, policy_name='BC_trained_policy_noiseless_wf', expert_avg_total_reward=expert_avg_total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d534062e",
      "metadata": {
        "id": "d534062e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "balloon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
